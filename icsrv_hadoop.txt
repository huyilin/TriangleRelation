***** RUNTIME ENVIRONMENT *****

Our cluster has ten blades and it runs a virtualized Solaris-based environment with 240 hardware threads (~=cores). Ten nodes are designated gateways dedicated for communication with the outside world (called “global zones” on Solaris), i.e. they are visible globally and can be connected to via ssh. These nodes are called icdatasrv[1|2|3|4|5|6|7|8|9|10].epfl.ch. 

Each global zone manages a blade and shares memory and I/O with 22 "local zones". These are virtual machines that each have a hardware thread exclusively assigned to them -- so work can run on each of these local zones in parallel. The names of these zones have the following format: icdatasrv[1-10]-[1-22]. Each local zone has 3GB of RAM assigned to it.

You can only start Hadoop jobs while logged into the cluster. Please log in to icdatasrv[1-10] except icdatasrv5 which is used for some Hadoop master processes. Local filesystems are not shared, so better always log in on the same machine.

Hadoop has been installed on icdatasrv* as follows. The system is configured with 88 worker nodes. You can use all of them, but you share them with the other teams. HDFS is Hadoop's distributed and replicated file system and the chunk size and the number of file replicas are set to their default values (the number of replicas is 3). The name server runs on icdatasrv5 and the jobtracker runs on icdatasrv5. Status information about namenode and jobtracker can be found at:

http://icdatasrv5:50070/   (namenode)
http://icdatasrv5:50030/   (job tracker)

It is possible (and not too difficult) to create a small Hadoop installation on your own computer/laptop for early-stage testing. Please follow the instructios at: 
* Section "Standalone Operation" from docs/quickstart.html, or
* Section "Setting up Hadoop on a single node" from http://wiki.apache.org/hadoop/GettingStartedWithHadoop
You can download the whole archive (including documentation) from
http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz
or at
http://hadoop.apache.org/docs/r1.2.1/api/.

***** HADOOP WORDCOUNT EXAMPLE *****
This tutorial is adapted from 
docs/mapred_tutorial.html from the archive
http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz.

1. To avoid entering password every time you log on to cluster, 
please read Section 'How to avoid entering password every time I access the cluster?' 
from https://github.com/epfldata/squall/wiki/EPFL-cluster-information.

2. Put hadoop executable in your path by executing the following command on the cluster:
  teamX@icdatasrv1 # for blade in {1..10}; do ssh teamX@icdatasrv$blade 'echo "PATH=\$PATH:/opt/hadoop/hadoop/bin/" >> .profile'; done
This adds the line:
  PATH=$PATH:/opt/hadoop/hadoop/bin/
at the end of each .profile file on the cluster.

3. Go to a server and make some directories:
  local@local # ssh teamX@icdatasrv1
  teamX@icdatasrv1 # mkdir wordcount
  teamX@icdatasrv1 # cd wordcount
  teamX@icdatasrv1 # mkdir classes
  teamX@icdatasrv1 # logout

4. Upload WordCount.java from your local machine. Go to the directory where the java file is located and then:
  local@local # scp WordCount.java teamX@icdatasrv1:wordcount

5. Go to the server again and compile the WordCount example:
  local@local # ssh teamX@icdatasrv1
  teamX@icdatasrv1 # cd wordcount
  teamX@icdatasrv1 # javac -cp /opt/hadoop/hadoop/hadoop-core-1.2.1.jar -d classes WordCount.java
  teamX@icdatasrv1 # jar cvf wordcount.jar -C classes/ .

6. In wordcount directory create input and output directories and fill the input directory with sample data:
  teamX@icdatasrv1 # mkdir ~/wordcount/input
  teamX@icdatasrv1 # mkdir ~/wordcount/output
  teamX@icdatasrv1 # echo "Hello World Bye World" > file01
  teamX@icdatasrv1 # echo "Hello Hadoop Goodbye Hadoop" > file02

7. The HDFS (Hadoop Distributed FileSystem) directory tree contains teamX (where X is your team number) directories 
on the top level – for instance, team3 owns the directory “/team3”. This is where you should put your files.
We will now create a subdirectory "input" under your team directory using:
  teamX@icdatasrv1 # hadoop dfs -mkdir /user/teamX/input
Now we add input files to HDFS:
  teamX@icdatasrv1 # hadoop dfs -put ~/wordcount/input/* /user/teamX/input
The files should appear on listing:
  teamX@icdatasrv1 # hadoop dfs -ls /user/teamX/input


Notes:
* All the files copied on HDFS are visible from all the blades
* To see other commands, run:
    teamX@icdatasrv1 # hadoop dfs
  or take a look at the archive at
    docs/hdfs_shell.html.
* Display files with -cat, export them to unix with -get, and delete them with -rm or -rmr. 

8. Finally, we can run the WordCount example:
  teamX@icdatasrv1 # cd ~/wordcount
  teamX@icdatasrv1 # hadoop jar wordcount.jar org.myorg.WordCount /user/teamX/input/ /user/teamX/output/
You can go on job tracker to see some statistics:
  http://icdatasrv5:50030/
The output can be downloaded by:
  hadoop dfs -get /teamX/output/* ~/wordcount/output/
Check if the output obtained is:
  cat ~/wordcount/output/part-00000
is
  Bye     1
  Goodbye 1
  Hadoop  2
  Hello   2
  World   2 

9. Obtain a list of examples possible to run directly from examples.jar:
		hadoop jar /opt/hadoop/hadoop/hadoop-examples-1.2.1.jar
	wordcount can be run by:
	 	hadoop jar /opt/hadoop/hadoop/hadoop-examples-1.2.1.jar wordcount /user/teamX/input /user/teamX/output
   Source code of these examples is available in src/examples/org/apache/hadoop/examples
     in the archive.

10. To change configuration (for example, the number of map and reduce tasks), use
      org.apache.hadoop.conf.Configuration or org.apache.hadoop.mapred.JobConf
    To see a list of configuration properties, look at:
      docs/mapred-default.html


***** REMARKS *****

In general, Hadoop programs can run a workflow of multiple jobs in a sequence.

Have a look at the information that is output at the end of the job execution. It includes information on how many map and reduce jobs were used. If there are surprisingly few, you were probably starting with too large input files and may want to split them into smaller chunks. Don’t expect great response times. Hadoop is always a bit sluggish – it is not strikingly efficient, which is annoying for small and simply jobs, but it is scalable.

It is important to behave socially while working on your project. If you post jobs to the system, your colleagues from other teams will wait for them to finish. So please test your code with small datasets; try to finish earlier than 24 hours before the deadline.  Try to run your experiments with sizeable data at times when you expect other teams are less likely to use the icdatasrv (e.g. at night). Only do experiments with large data in the end, after your implementation has reached maturity and has been well tested on small datasets.


***** MORE INFORMATION *****
* Full documentation - docs/index.html from the archive
* Complete class reference - docs/api/index.html from the archive
* More tutorials - http://hadooptutorial.wikispaces.com/Hadoop
